{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.helpers import get_data, gridsearch_with_output, score_classifer, test_classifer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, confusion_matrix\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_analysis.pop('fraud')\n",
    "X = df_analysis.copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, shuffle=True, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:   34.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result of gridsearch:\n",
      "Parameter            | Optimal  | Gridsearch values\n",
      "-------------------------------------------------------\n",
      "max_depth            | 2        | [2, 4, None]\n",
      "max_features         | sqrt     | ['sqrt', None]\n",
      "oob_score            | True     | [True, False]\n",
      "n_estimators         | 20       | [20, 30]\n",
      "class_weight         | balanced | ['balanced', None]\n",
      "random_state         | 1        | [1]\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Grid Search Tuning\n",
    "random_forest_grid = {'max_depth': [2, 4, None],\n",
    "                      'max_features': ['sqrt', None],\n",
    "                      'oob_score': [True, False],\n",
    "                      'n_estimators': [20, 30],\n",
    "                      'class_weight': ['balanced', None],\n",
    "                      'random_state': [1]\n",
    "                     }\n",
    "rf_best_params, rf_best_model, rf_best_score = gridsearch_with_output(RandomForestClassifier(), \n",
    "                                                                      random_forest_grid, \n",
    "                                                                      'recall',\n",
    "                                                                      X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9252526439482962"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   33.8s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result of gridsearch:\n",
      "Parameter            | Optimal  | Gridsearch values\n",
      "-------------------------------------------------------\n",
      "max_depth            | None     | [4, None]\n",
      "max_features         | sqrt     | ['sqrt', None]\n",
      "min_samples_split    | 4        | [2, 4, None]\n",
      "min_samples_leaf     | 1        | [1, None]\n",
      "n_estimators         | 50       | [40, 50]\n",
      "learning_rate        | 0.5      | [0.5, 0.6]\n",
      "random_state         | 1        | [1]\n"
     ]
    }
   ],
   "source": [
    "#  Gradient Boost Grid Search\n",
    "gb_grid = {'max_depth': [4, None],\n",
    "           'max_features': ['sqrt', None],\n",
    "           'min_samples_split': [2, 4, None],\n",
    "           'min_samples_leaf': [1, None],\n",
    "           'n_estimators': [40, 50],\n",
    "           'learning_rate': [.5, .6],\n",
    "           'random_state': [1]\n",
    "                     }\n",
    "\n",
    "gb_best_params, gb_best_model, gb_best_score = gridsearch_with_output(GradientBoostingClassifier(), \n",
    "                                                           gb_grid, \n",
    "                                                           'f1',\n",
    "                                                           X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8730176989500246"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   49.4s finished\n",
      "/Users/isabella/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:17:45] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      "Result of gridsearch:\n",
      "Parameter            | Optimal  | Gridsearch values\n",
      "-------------------------------------------------------\n",
      "max_depth            | None     | [4, None]\n",
      "min_child_weight     | 0.1      | [0.1, None]\n",
      "learning_rate        | 0.4      | [0.4, 0.5]\n",
      "random_state         | 1        | [1]\n"
     ]
    }
   ],
   "source": [
    "xgb_grid = {'max_depth': [4, None],\n",
    "            'min_child_weight': [.1, None],\n",
    "           'learning_rate': [.4, .5],\n",
    "           'random_state': [1]\n",
    "                     }\n",
    "\n",
    "xgb_best_params, xgb_best_model, xgb_best_score = gridsearch_with_output(XGBClassifier(), \n",
    "                                                           xgb_grid, \n",
    "                                                           'f1',\n",
    "                                                           X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8844152152975102"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(n_jobs=-1, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabella/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:41:22] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[['rf',\n",
       "                              RandomForestClassifier(class_weight='balanced',\n",
       "                                                     max_depth=2,\n",
       "                                                     max_features='sqrt',\n",
       "                                                     n_estimators=20,\n",
       "                                                     oob_score=True,\n",
       "                                                     random_state=1)],\n",
       "                             ['xgb',\n",
       "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                            colsample_bylevel=1,\n",
       "                                            colsample_bynode=1,\n",
       "                                            colsample_bytree=1, gamma=0,\n",
       "                                            gpu_id=-1, importance_type='gain',\n",
       "                                            interaction_constraints='',\n",
       "                                            learning_...\n",
       "                                            monotone_constraints='()',\n",
       "                                            n_estimators=100, n_jobs=12,\n",
       "                                            num_parallel_tree=1, random_state=1,\n",
       "                                            reg_alpha=0, reg_lambda=1,\n",
       "                                            scale_pos_weight=1, subsample=1,\n",
       "                                            tree_method='exact',\n",
       "                                            validate_parameters=1,\n",
       "                                            verbosity=None)],\n",
       "                             ['gbc',\n",
       "                              GradientBoostingClassifier(learning_rate=0.5,\n",
       "                                                         max_depth=None,\n",
       "                                                         max_features='sqrt',\n",
       "                                                         min_samples_split=4,\n",
       "                                                         n_estimators=50,\n",
       "                                                         random_state=1)]],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate classifiers\n",
    "rf = rf_best_model\n",
    "xgb = xgb_best_model\n",
    "gbc = gb_best_model\n",
    "\n",
    "# instantiate voting classifier after other estimators have already been fit\n",
    "estimators = [['rf',rf], ['xgb', xgb], ['gbc', gbc]]\n",
    "voting = VotingClassifier(estimators, voting = 'soft')\n",
    "voting.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = voting.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "precision: 0.3511326860841424\n",
      "recall: 0.9393939393939394\n",
      "f1: 0.5111896348645465\n",
      "confusion matrix: \n",
      " [[2189  401]\n",
      " [  14  217]] \n",
      " \n",
      "XGBoost\n",
      "precision: 0.9414634146341463\n",
      "recall: 0.8354978354978355\n",
      "f1: 0.8853211009174311\n",
      "confusion matrix: \n",
      " [[2578   12]\n",
      " [  38  193]] \n",
      " \n",
      "Gradient Boost\n",
      "precision: 0.9631578947368421\n",
      "recall: 0.7922077922077922\n",
      "f1: 0.8693586698337292\n",
      "confusion matrix: \n",
      " [[2583    7]\n",
      " [  48  183]] \n",
      " \n",
      "Voting\n",
      "precision: 0.9593908629441624\n",
      "recall: 0.8181818181818182\n",
      "f1: 0.883177570093458\n",
      "confusion matrix: \n",
      " [[2582    8]\n",
      " [  42  189]] \n",
      " \n"
     ]
    }
   ],
   "source": [
    "rf = rf_best_model\n",
    "y_predict = rf.predict(X_test)\n",
    "print(\"Random Forest\")\n",
    "print(f\"precision: {precision_score(y_test, y_predict)}\")\n",
    "print(f\"recall: {recall_score(y_test, y_predict)}\")\n",
    "print(f\"f1: {f1_score(y_test, y_predict)}\")\n",
    "print(f\"confusion matrix: \\n {confusion_matrix(y_test, y_predict)} \\n \")\n",
    "\n",
    "xgb = xgb_best_model\n",
    "y_predict = xgb.predict(X_test)\n",
    "print(\"XGBoost\")\n",
    "print(f\"precision: {precision_score(y_test, y_predict)}\")\n",
    "print(f\"recall: {recall_score(y_test, y_predict)}\")\n",
    "print(f\"f1: {f1_score(y_test, y_predict)}\")\n",
    "print(f\"confusion matrix: \\n {confusion_matrix(y_test, y_predict)} \\n \")\n",
    "\n",
    "gb = gb_best_model\n",
    "y_predict = gb.predict(X_test)\n",
    "print(\"Gradient Boost\")\n",
    "print(f\"precision: {precision_score(y_test, y_predict)}\")\n",
    "print(f\"recall: {recall_score(y_test, y_predict)}\")\n",
    "print(f\"f1: {f1_score(y_test, y_predict)}\")\n",
    "print(f\"confusion matrix: \\n {confusion_matrix(y_test, y_predict)} \\n \")\n",
    "\n",
    "\n",
    "y_predict = voting.predict(X_test)\n",
    "print(\"Voting\")\n",
    "print(f\"precision: {precision_score(y_test, y_predict)}\")\n",
    "print(f\"recall: {recall_score(y_test, y_predict)}\")\n",
    "print(f\"f1: {f1_score(y_test, y_predict)}\")\n",
    "print(f\"confusion matrix: \\n {confusion_matrix(y_test, y_predict)} \\n \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'voting_model.sav'\n",
    "pickle.dump(voting, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
