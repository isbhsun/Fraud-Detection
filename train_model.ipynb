{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.helpers import get_data, gridsearch_with_output, score_classifer, test_classifer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, confusion_matrix\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_analysis.pop('fraud')\n",
    "X = df_analysis.copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=.2,\n",
    "                                                    stratify=y,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   20.7s\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:   34.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result of gridsearch:\n",
      "Parameter            | Optimal  | Gridsearch values\n",
      "-------------------------------------------------------\n",
      "max_depth            | 4        | [2, 4, None]\n",
      "max_features         | sqrt     | ['sqrt', None]\n",
      "oob_score            | True     | [True, False]\n",
      "n_estimators         | 20       | [20, 30]\n",
      "class_weight         | balanced | ['balanced', None]\n",
      "random_state         | 1        | [1]\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Grid Search Tuning\n",
    "random_forest_grid = {'max_depth': [2, 4, None],\n",
    "                      'max_features': ['sqrt', None],\n",
    "                      'oob_score': [True, False],\n",
    "                      'n_estimators': [20, 30],\n",
    "                      'class_weight': ['balanced', None],\n",
    "                      'random_state': [1]\n",
    "                     }\n",
    "rf_best_params, rf_best_model, rf_best_score = gridsearch_with_output(RandomForestClassifier(), \n",
    "                                                                      random_forest_grid, \n",
    "                                                                      'recall',\n",
    "                                                                      X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. sale_duration2 (0.190222)\n",
      "2. org_facebook (0.103722)\n",
      "3. user_age (0.099429)\n",
      "4. user_type_1 (0.089524)\n",
      "5. num_payouts (0.069148)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9068037602820211"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=5\n",
    "importances = rf_best_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = list(X.columns[indices])\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(n):\n",
    "    print(\"%d. %s (%f)\" % (f + 1, features[f], importances[indices[f]]))\n",
    "    \n",
    "rf_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   32.2s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result of gridsearch:\n",
      "Parameter            | Optimal  | Gridsearch values\n",
      "-------------------------------------------------------\n",
      "max_depth            | None     | [4, None]\n",
      "max_features         | sqrt     | ['sqrt', None]\n",
      "min_samples_split    | 4        | [2, 4, None]\n",
      "min_samples_leaf     | 1        | [1, None]\n",
      "n_estimators         | 50       | [40, 50]\n",
      "learning_rate        | 0.6      | [0.5, 0.6]\n",
      "random_state         | 1        | [1]\n"
     ]
    }
   ],
   "source": [
    "#  Gradient Boost Grid Search\n",
    "gb_grid = {'max_depth': [4, None],\n",
    "           'max_features': ['sqrt', None],\n",
    "           'min_samples_split': [2, 4, None],\n",
    "           'min_samples_leaf': [1, None],\n",
    "           'n_estimators': [40, 50],\n",
    "           'learning_rate': [.5, .6],\n",
    "           'random_state': [1]\n",
    "                     }\n",
    "\n",
    "gb_best_params, gb_best_model, gb_best_score = gridsearch_with_output(GradientBoostingClassifier(), \n",
    "                                                           gb_grid, \n",
    "                                                           'f1',\n",
    "                                                           X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. payout_type_ (0.231896)\n",
      "2. sale_duration2 (0.214292)\n",
      "3. user_age (0.093404)\n",
      "4. org_facebook (0.057802)\n",
      "5. gts (0.048919)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8710500032832165"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=5\n",
    "importances = gb_best_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = list(X.columns[indices])\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(n):\n",
    "    print(\"%d. %s (%f)\" % (f + 1, features[f], importances[indices[f]]))\n",
    "    \n",
    "gb_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   55.9s finished\n",
      "/Users/isabella/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:56:08] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      "Result of gridsearch:\n",
      "Parameter            | Optimal  | Gridsearch values\n",
      "-------------------------------------------------------\n",
      "max_depth            | None     | [4, None]\n",
      "min_child_weight     | 0.1      | [0.1, None]\n",
      "learning_rate        | 0.5      | [0.4, 0.5]\n",
      "random_state         | 1        | [1]\n"
     ]
    }
   ],
   "source": [
    "xgb_grid = {'max_depth': [4, None],\n",
    "            'min_child_weight': [.1, None],\n",
    "           'learning_rate': [.4, .5],\n",
    "           'random_state': [1]\n",
    "                     }\n",
    "\n",
    "xgb_best_params, xgb_best_model, xgb_best_score = gridsearch_with_output(XGBClassifier(), \n",
    "                                                           xgb_grid, \n",
    "                                                           'f1',\n",
    "                                                           X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. payout_type_ (0.170804)\n",
      "2. payout_type_CHECK (0.061795)\n",
      "3. sale_duration2 (0.061306)\n",
      "4. country_PR (0.058081)\n",
      "5. num_payouts (0.052095)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8826067100755397"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=5\n",
    "importances = xgb_best_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = list(X.columns[indices])\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(n):\n",
    "    print(\"%d. %s (%f)\" % (f + 1, features[f], importances[indices[f]]))\n",
    "    \n",
    "xgb_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8826067100755397"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(n_jobs=-1, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabella/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:14:15] WARNING: /Users/runner/miniforge3/conda-bld/xgboost_1607604592557/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[['rf',\n",
       "                              RandomForestClassifier(class_weight='balanced',\n",
       "                                                     max_depth=4,\n",
       "                                                     max_features='sqrt',\n",
       "                                                     n_estimators=20,\n",
       "                                                     oob_score=True,\n",
       "                                                     random_state=1)],\n",
       "                             ['xgb',\n",
       "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                            colsample_bylevel=1,\n",
       "                                            colsample_bynode=1,\n",
       "                                            colsample_bytree=1, gamma=0,\n",
       "                                            gpu_id=-1, importance_type='gain',\n",
       "                                            interaction_constraints='',\n",
       "                                            learning_...\n",
       "                                            monotone_constraints='()',\n",
       "                                            n_estimators=100, n_jobs=12,\n",
       "                                            num_parallel_tree=1, random_state=1,\n",
       "                                            reg_alpha=0, reg_lambda=1,\n",
       "                                            scale_pos_weight=1, subsample=1,\n",
       "                                            tree_method='exact',\n",
       "                                            validate_parameters=1,\n",
       "                                            verbosity=None)],\n",
       "                             ['gbc',\n",
       "                              GradientBoostingClassifier(learning_rate=0.6,\n",
       "                                                         max_depth=None,\n",
       "                                                         max_features='sqrt',\n",
       "                                                         min_samples_split=4,\n",
       "                                                         n_estimators=50,\n",
       "                                                         random_state=1)]],\n",
       "                 voting='soft', weights=[3, 2, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate classifiers\n",
    "rf = rf_best_model\n",
    "xgb = xgb_best_model\n",
    "gbc = gb_best_model\n",
    "\n",
    "# instantiate voting classifier after other estimators have already been fit\n",
    "estimators = [['rf',rf], ['xgb', xgb], ['gbc', gbc]]\n",
    "voting = VotingClassifier(estimators, voting = 'soft', weights=[3,2,1])\n",
    "voting.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = voting.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "precision: 0.5363408521303258\n",
      "recall: 0.9264069264069265\n",
      "f1: 0.6793650793650794\n",
      "confusion matrix: \n",
      " [[2405  185]\n",
      " [  17  214]] \n",
      " \n",
      "XGBoost\n",
      "precision: 0.9320388349514563\n",
      "recall: 0.8311688311688312\n",
      "f1: 0.8787185354691076\n",
      "confusion matrix: \n",
      " [[2576   14]\n",
      " [  39  192]] \n",
      " \n",
      "Gradient Boost\n",
      "precision: 0.9583333333333334\n",
      "recall: 0.7965367965367965\n",
      "f1: 0.8699763593380614\n",
      "confusion matrix: \n",
      " [[2582    8]\n",
      " [  47  184]] \n",
      " \n",
      "Voting\n",
      "precision: 0.933649289099526\n",
      "recall: 0.8528138528138528\n",
      "f1: 0.8914027149321267\n",
      "confusion matrix: \n",
      " [[2576   14]\n",
      " [  34  197]] \n",
      " \n"
     ]
    }
   ],
   "source": [
    "rf = rf_best_model\n",
    "y_predict = rf.predict(X_test)\n",
    "print(\"Random Forest\")\n",
    "print(f\"precision: {precision_score(y_test, y_predict)}\")\n",
    "print(f\"recall: {recall_score(y_test, y_predict)}\")\n",
    "print(f\"f1: {f1_score(y_test, y_predict)}\")\n",
    "print(f\"confusion matrix: \\n {confusion_matrix(y_test, y_predict)} \\n \")\n",
    "\n",
    "xgb = xgb_best_model\n",
    "y_predict = xgb.predict(X_test)\n",
    "print(\"XGBoost\")\n",
    "print(f\"precision: {precision_score(y_test, y_predict)}\")\n",
    "print(f\"recall: {recall_score(y_test, y_predict)}\")\n",
    "print(f\"f1: {f1_score(y_test, y_predict)}\")\n",
    "print(f\"confusion matrix: \\n {confusion_matrix(y_test, y_predict)} \\n \")\n",
    "\n",
    "gb = gb_best_model\n",
    "y_predict = gb.predict(X_test)\n",
    "print(\"Gradient Boost\")\n",
    "print(f\"precision: {precision_score(y_test, y_predict)}\")\n",
    "print(f\"recall: {recall_score(y_test, y_predict)}\")\n",
    "print(f\"f1: {f1_score(y_test, y_predict)}\")\n",
    "print(f\"confusion matrix: \\n {confusion_matrix(y_test, y_predict)} \\n \")\n",
    "\n",
    "\n",
    "y_predict = voting.predict(X_test)\n",
    "print(\"Voting\")\n",
    "print(f\"precision: {precision_score(y_test, y_predict)}\")\n",
    "print(f\"recall: {recall_score(y_test, y_predict)}\")\n",
    "print(f\"f1: {f1_score(y_test, y_predict)}\")\n",
    "print(f\"confusion matrix: \\n {confusion_matrix(y_test, y_predict)} \\n \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'voting_model.sav'\n",
    "pickle.dump(voting, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
